{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "java version \"1.8.0_161\"\r\n",
      "Java(TM) SE Runtime Environment (build 1.8.0_161-b12)\r\n",
      "Java HotSpot(TM) 64-Bit Server VM (build 25.161-b12, mixed mode)\r\n"
     ]
    }
   ],
   "source": [
    "!java -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.6.3 :: Anaconda custom (64-bit)\r\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install Pyspark using pip\n",
    "If you have an old installation of spark which involved setting up environment variables in your .bash_profile, you will need to clear those as they will affect the running of this new pyspark installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /Users/kishan/anaconda3/lib/python3.6/site-packages (2.3.2)\n",
      "Requirement already satisfied: py4j==0.10.7 in /Users/kishan/anaconda3/lib/python3.6/site-packages (from pyspark) (0.10.7)\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDDs and DataFrames\n",
    "\n",
    "* Creating RDDs and DataFrames using SparkContext\n",
    "* Interoperability between RDDs and DataFrames\n",
    "* Multiple rows and multiple column specifications for DataFrames\n",
    "* Creating DataFrames using SQLContext\n",
    "* Selecting, editing and renaming columns in dataframes\n",
    "* Interoperability between Pandas and Spark dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.112:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = SparkContext()\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import Row\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating RDDs using sc.parallelize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We effectively create one row of data storing information about a car in a dealership:\n",
    "* the first element is the index \n",
    "* the second element is the name of the car\n",
    "* thirdly, we store the quantity of cars in our inventory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:194"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_data = sc.parallelize([1, \"Nissan Versa\", 12])\n",
    "\n",
    "simple_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The count() function\n",
    "This returns the number of elements in the RDD. Each element in the list is now an element of the RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The first() function\n",
    "This returns the first element of the RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_data.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The take() function\n",
    "This will return the first n elements of the RDD where n is the argument passed to the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 'Nissan Versa']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_data.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The collect() function\n",
    "Returns a list containing all the elements in the RDD. We get a 1D list with the 3 elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 'Nissan Versa', 12]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_data.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert the RDD to a DataFrame\n",
    "Converting the current RDD to a DataFrame results in an error as it's 1-dimensional\n",
    "\n",
    "* This RDD does not have \"columns\", it cannot be represented as a tabular data frame\n",
    "* DataFrames are structured datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Can not infer schema for type: <class 'int'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-88e2b3e8db63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msimple_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36mtoDF\u001b[0;34m(self, schema, sampleRatio)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Alice'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \"\"\"\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampleRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoDF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoDF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 689\u001b[0;31m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    690\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_createFromRDD\u001b[0;34m(self, rdd, schema, samplingRatio)\u001b[0m\n\u001b[1;32m    382\u001b[0m         \"\"\"\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m             \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inferSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m             \u001b[0mconverter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_converter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstruct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0mrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_inferSchema\u001b[0;34m(self, rdd, samplingRatio, names)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msamplingRatio\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m             \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_infer_schema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0m_has_nulltype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyspark/sql/types.py\u001b[0m in \u001b[0;36m_infer_schema\u001b[0;34m(row, names)\u001b[0m\n\u001b[1;32m   1092\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1093\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1094\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Can not infer schema for type: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1095\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1096\u001b[0m     \u001b[0mfields\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mStructField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_infer_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Can not infer schema for type: <class 'int'>"
     ]
    }
   ],
   "source": [
    "df = simple_data.toDF()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RDDs with records using sc.parallelize()\n",
    "We effectively create two rows of data by passing in two lists to sc.parallelize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[8] at parallelize at PythonRDD.scala:194"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records = sc.parallelize([[1, \"Nissan Versa\", 12],\n",
    "                          [2, \"Ford Fiesta\", 9]])\n",
    "records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The number of records corresponds to the number of rows\n",
    "Even though we have twice as much data in this RDD as in the previous one we created, the count() returns only two elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The RDD is now effectively a list of lists\n",
    "While the previous RDD contained a 1D list, this one contains two elements each of which is a 1D list - giving a 2D list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 'Nissan Versa', 12], [2, 'Ford Fiesta', 9]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The first element is a list\n",
    "Effectively the first \"row\" in the RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 'Nissan Versa', 12]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 'Nissan Versa', 12], [2, 'Ford Fiesta', 9]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This RDD can be converted to a DataFrame\n",
    "This RDD does have \"columns\", it can be represented as a tabular data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = records.toDF()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The DataFrame has automatically detected the types of the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_1: bigint, _2: string, _3: bigint]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The show() function for a DataFrame\n",
    "By default it returns the first 20 rows of the DataFrame. We can pass in the number of rows to return as an argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+---+\n",
      "| _1|          _2| _3|\n",
      "+---+------------+---+\n",
      "|  1|Nissan Versa| 12|\n",
      "|  2| Ford Fiesta|  9|\n",
      "+---+------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating dataframes using sc.parallelize() and Row() functions\n",
    "Row functions allow specifying column names for dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[24] at parallelize at PythonRDD.scala:194"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = sc.parallelize([Row(index = 1,\n",
    "                           vehicle_name = \"Nissan Versa\",\n",
    "                           vehicle_stock = 12)])\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This RDD contains a single Row element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(index=1, vehicle_name='Nissan Versa', vehicle_stock=12)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+-------------+\n",
      "|index|vehicle_name|vehicle_stock|\n",
      "+-----+------------+-------------+\n",
      "|    1|Nissan Versa|           12|\n",
      "+-----+------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = data.toDF()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Working with multiple rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[36] at parallelize at PythonRDD.scala:194"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = sc.parallelize([Row(index = 1,\n",
    "                       vehicle_name = \"Nissan Versa\",\n",
    "                       vehicle_stock = 12\n",
    "                       ),\n",
    "                       Row(index = 2,\n",
    "                       vehicle_name = \"Ford Fiesta\",\n",
    "                       vehicle_stock = 9\n",
    "                       ),\n",
    "                       Row(index = 3,\n",
    "                       vehicle_name = \"Hyundai Accent\",\n",
    "                       vehicle_stock = 8)])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+-------------+\n",
      "|index|  vehicle_name|vehicle_stock|\n",
      "+-----+--------------+-------------+\n",
      "|    1|  Nissan Versa|           12|\n",
      "|    2|   Ford Fiesta|            9|\n",
      "|    3|Hyundai Accent|            8|\n",
      "+-----+--------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = data.toDF()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple columns with complex data types\n",
    "Here, we highlight many of the different data types which we can store in an RDD. We start with a string, float and integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "complex_data = sc.parallelize([Row(\n",
    "                               col_string = \"Alice\",\n",
    "                               col_double = 3.14,\n",
    "                               col_integer = 20)\n",
    "                              ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+----------+\n",
      "|col_double|col_integer|col_string|\n",
      "+----------+-----------+----------+\n",
      "|      3.14|         20|     Alice|\n",
      "+----------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "complex_data_df = complex_data.toDF()\n",
    "complex_data_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[col_double: double, col_integer: bigint, col_string: string]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complex_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "complex_data = sc.parallelize([Row(\n",
    "                               col_string = \"Alice\",\n",
    "                               col_double = 3.14,\n",
    "                               col_integer = 20,\n",
    "                               col_boolean = True,\n",
    "                               col_list = [2,4,6])\n",
    "                              ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-----------+---------+----------+\n",
      "|col_boolean|col_double|col_integer| col_list|col_string|\n",
      "+-----------+----------+-----------+---------+----------+\n",
      "|       true|      3.14|         20|[2, 4, 6]|     Alice|\n",
      "+-----------+----------+-----------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "complex_data_df = complex_data.toDF()\n",
    "complex_data_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[col_boolean: boolean, col_double: double, col_integer: bigint, col_list: array<bigint>, col_string: string]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complex_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple rows with complex data types\n",
    "Our RDDs can also contain lists, dictionaries, Row types and timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "complex_data = sc.parallelize([Row(\n",
    "                                col_list = [2,4,6],\n",
    "                                col_dict = {\"a1\": 0},\n",
    "                                col_row = Row(x=15, y=25, z=35),\n",
    "                                col_time = datetime(2018, 7, 1, 14, 1, 2)\n",
    "                            ),              \n",
    "                            Row(\n",
    "                                col_list = [2,4,6,8,10], \n",
    "                                col_dict = {\"a1\": 0,\"a2\": 1 }, \n",
    "                                col_row = Row(x=45, y=55, z=65),\n",
    "                                col_time = datetime(2018, 7, 2, 14, 1, 3)\n",
    "                            ),\n",
    "                            Row(\n",
    "                                col_list = [2,4,6,8,10,12,14], \n",
    "                                col_dict = {\"a1\": 0, \"a2\": 1, \"a3\": 2 }, \n",
    "                                col_row = Row(x=75, y=85, z=95),\n",
    "                                col_time = datetime(2018, 7, 3, 14, 1, 4)\n",
    "                            )]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------------+-------------------+\n",
      "|            col_dict|            col_list|     col_row|           col_time|\n",
      "+--------------------+--------------------+------------+-------------------+\n",
      "|           [a1 -> 0]|           [2, 4, 6]|[15, 25, 35]|2018-07-01 14:01:02|\n",
      "|  [a1 -> 0, a2 -> 1]|    [2, 4, 6, 8, 10]|[45, 55, 65]|2018-07-02 14:01:03|\n",
      "|[a1 -> 0, a2 -> 1...|[2, 4, 6, 8, 10, ...|[75, 85, 95]|2018-07-03 14:01:04|\n",
      "+--------------------+--------------------+------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "complex_data_df = complex_data.toDF()\n",
    "complex_data_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[col_dict: map<string,bigint>, col_list: array<bigint>, col_row: struct<x:bigint,y:bigint,z:bigint>, col_time: timestamp]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complex_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating DataFrames using SQLContext\n",
    "\n",
    "SQLContext can create dataframes directly from raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.context.SQLContext at 0x112473f98>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The SQLContext range() function returns a DataFrame\n",
    "This creates a column of 4 integers from 0-3. The name of this column is set to \"id\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = sqlContext.range(4)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rows specified in tuples\n",
    "SQLContext DataFrames are specified in the form a list of tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = [('Nissan Versa', 12),\n",
    "        ('Ford Fiesta', 9),\n",
    "        ('Hyundai Accent', 8)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The DataFrame is created without column headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---+\n",
      "|            _1| _2|\n",
      "+--------------+---+\n",
      "|  Nissan Versa| 12|\n",
      "|   Ford Fiesta|  9|\n",
      "|Hyundai Accent|  8|\n",
      "+--------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.createDataFrame(data).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set the column names when creating the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------+\n",
      "|  vehicle_name|vehicle_stock|\n",
      "+--------------+-------------+\n",
      "|  Nissan Versa|           12|\n",
      "|   Ford Fiesta|            9|\n",
      "|Hyundai Accent|            8|\n",
      "+--------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.createDataFrame(data, \n",
    "                           ['vehicle_name', 'vehicle_stock']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding more data to the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "complex_data = [\n",
    "                 (1.0,\n",
    "                  12,\n",
    "                  \"Nissan Versa\", \n",
    "                  True, \n",
    "                  [2,4,6], \n",
    "                  {\"a1\": 0},\n",
    "                  Row(x=1, y=2, z=3), \n",
    "                  datetime(2018, 7, 1, 14, 1, 2)),\n",
    "\n",
    "                 (2.0,\n",
    "                  13,\n",
    "                  \"Ford Fiesta\", \n",
    "                  True, \n",
    "                  [2,4,6,8,10], \n",
    "                  {\"a1\": 0,\"a2\": 1 }, \n",
    "                  Row(x=1, y=2, z=3), \n",
    "                  datetime(2018, 7, 2, 14, 1, 3)),\n",
    "\n",
    "                  (3.0,\n",
    "                   15,\n",
    "                   \"Hyundai Accent\", \n",
    "                   True, \n",
    "                   [2,4,6,8,10,12,14], \n",
    "                   {\"a1\": 0, \"a2\": 1, \"a3\": 2 }, \n",
    "                   Row(x=1, y=2, z=3), \n",
    "                   datetime(2018, 7, 3, 14, 1, 4))\n",
    "                ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+--------------+----+--------------------+--------------------+---------+-------------------+\n",
      "| _1| _2|            _3|  _4|                  _5|                  _6|       _7|                 _8|\n",
      "+---+---+--------------+----+--------------------+--------------------+---------+-------------------+\n",
      "|1.0| 12|  Nissan Versa|true|           [2, 4, 6]|           [a1 -> 0]|[1, 2, 3]|2018-07-01 14:01:02|\n",
      "|2.0| 13|   Ford Fiesta|true|    [2, 4, 6, 8, 10]|  [a1 -> 0, a2 -> 1]|[1, 2, 3]|2018-07-02 14:01:03|\n",
      "|3.0| 15|Hyundai Accent|true|[2, 4, 6, 8, 10, ...|[a1 -> 0, a2 -> 1...|[1, 2, 3]|2018-07-03 14:01:04|\n",
      "+---+---+--------------+----+--------------------+--------------------+---------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.createDataFrame(complex_data).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+--------------+--------+--------------------+--------------------+---------+-------------------+\n",
      "|col_int|col_double|    col_string|col_bool|           col_array|      col_dictionary|  col_row|      col_date_time|\n",
      "+-------+----------+--------------+--------+--------------------+--------------------+---------+-------------------+\n",
      "|    1.0|        12|  Nissan Versa|    true|           [2, 4, 6]|           [a1 -> 0]|[1, 2, 3]|2018-07-01 14:01:02|\n",
      "|    2.0|        13|   Ford Fiesta|    true|    [2, 4, 6, 8, 10]|  [a1 -> 0, a2 -> 1]|[1, 2, 3]|2018-07-02 14:01:03|\n",
      "|    3.0|        15|Hyundai Accent|    true|[2, 4, 6, 8, 10, ...|[a1 -> 0, a2 -> 1...|[1, 2, 3]|2018-07-03 14:01:04|\n",
      "+-------+----------+--------------+--------+--------------------+--------------------+---------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "complex_data_df = sqlContext.createDataFrame(complex_data, [\n",
    "                                             'col_int',\n",
    "                                             'col_double',\n",
    "                                             'col_string',\n",
    "                                             'col_bool',\n",
    "                                             'col_array',\n",
    "                                             'col_dictionary',\n",
    "                                             'col_row',\n",
    "                                             'col_date_time']\n",
    "                                            )\n",
    "complex_data_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating dataframes using SQL Context and the Row function\n",
    "Row functions can be used without specifying column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = sc.parallelize([\n",
    "                        Row(1, \"Nissan Versa\", 12),\n",
    "                        Row(2, \"Ford Fiesta\", 9),\n",
    "                        Row(3, \"Hyundai Accent\", 8)\n",
    "                     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+---+\n",
      "| _1|            _2| _3|\n",
      "+---+--------------+---+\n",
      "|  1|  Nissan Versa| 12|\n",
      "|  2|   Ford Fiesta|  9|\n",
      "|  3|Hyundai Accent|  8|\n",
      "+---+--------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.toDF().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a new Row with the column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "column_names = Row('index', 'vehicle_name', 'vehicle_stock')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The RDD's map() function\n",
    "Return a new RDD by applying a function to each element of this RDD. Here, we call column_names for every row of the RDD with the row elements passed as positional arguments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[139] at RDD at PythonRDD.scala:52"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cars = data.map(lambda r: column_names(*r))\n",
    "\n",
    "cars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This has the effect of setting column headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(index=1, vehicle_name='Nissan Versa', vehicle_stock=12),\n",
       " Row(index=2, vehicle_name='Ford Fiesta', vehicle_stock=9),\n",
       " Row(index=3, vehicle_name='Hyundai Accent', vehicle_stock=8)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cars.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[index: bigint, vehicle_name: string, vehicle_stock: bigint]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cars_df = sqlContext.createDataFrame(cars)\n",
    "\n",
    "cars_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+-------------+\n",
      "|index|  vehicle_name|vehicle_stock|\n",
      "+-----+--------------+-------------+\n",
      "|    1|  Nissan Versa|           12|\n",
      "|    2|   Ford Fiesta|            9|\n",
      "|    3|Hyundai Accent|            8|\n",
      "+-----+--------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cars_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting specific rows from dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(col_int=1.0, col_double=12, col_string='Nissan Versa', col_bool=True, col_array=[2, 4, 6], col_dictionary={'a1': 0}, col_row=Row(x=1, y=2, z=3), col_date_time=datetime.datetime(2018, 7, 1, 14, 1, 2))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complex_data_df.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(col_int=1.0, col_double=12, col_string='Nissan Versa', col_bool=True, col_array=[2, 4, 6], col_dictionary={'a1': 0}, col_row=Row(x=1, y=2, z=3), col_date_time=datetime.datetime(2018, 7, 1, 14, 1, 2)),\n",
       " Row(col_int=2.0, col_double=13, col_string='Ford Fiesta', col_bool=True, col_array=[2, 4, 6, 8, 10], col_dictionary={'a1': 0, 'a2': 1}, col_row=Row(x=1, y=2, z=3), col_date_time=datetime.datetime(2018, 7, 2, 14, 1, 3))]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complex_data_df.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting specific cells from dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nissan Versa'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cell_string = complex_data_df.collect()[0][2]\n",
    "\n",
    "cell_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4, 6]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cell_list = complex_data_df.collect()[0][4]\n",
    "\n",
    "cell_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The data returned is a copy of the DataFrame contents\n",
    "This is an independent copy. If we append a value to the returned list, it does not modify the contents of the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4, 6, 100]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cell_list.append(100)\n",
    "\n",
    "cell_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+--------------+--------+--------------------+--------------------+---------+-------------------+\n",
      "|col_int|col_double|    col_string|col_bool|           col_array|      col_dictionary|  col_row|      col_date_time|\n",
      "+-------+----------+--------------+--------+--------------------+--------------------+---------+-------------------+\n",
      "|    1.0|        12|  Nissan Versa|    true|           [2, 4, 6]|           [a1 -> 0]|[1, 2, 3]|2018-07-01 14:01:02|\n",
      "|    2.0|        13|   Ford Fiesta|    true|    [2, 4, 6, 8, 10]|  [a1 -> 0, a2 -> 1]|[1, 2, 3]|2018-07-02 14:01:03|\n",
      "|    3.0|        15|Hyundai Accent|    true|[2, 4, 6, 8, 10, ...|[a1 -> 0, a2 -> 1...|[1, 2, 3]|2018-07-03 14:01:04|\n",
      "+-------+----------+--------------+--------+--------------------+--------------------+---------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "complex_data_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting specific columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One way is to use the rdd.map() operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Nissan Versa', {'a1': 0}),\n",
       " ('Ford Fiesta', {'a1': 0, 'a2': 1}),\n",
       " ('Hyundai Accent', {'a1': 0, 'a2': 1, 'a3': 2})]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complex_data_df.rdd\\\n",
    "               .map(lambda x: (x.col_string, x.col_dictionary))\\\n",
    "               .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The DataFrame select() function is another option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+-------------------+\n",
      "|    col_string|           col_array|      col_date_time|\n",
      "+--------------+--------------------+-------------------+\n",
      "|  Nissan Versa|           [2, 4, 6]|2018-07-01 14:01:02|\n",
      "|   Ford Fiesta|    [2, 4, 6, 8, 10]|2018-07-02 14:01:03|\n",
      "|Hyundai Accent|[2, 4, 6, 8, 10, ...|2018-07-03 14:01:04|\n",
      "+--------------+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "complex_data_df.select(\n",
    "                        'col_string',\n",
    "                        'col_array',\n",
    "                        'col_date_time'\n",
    "                      ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Editing column data\n",
    "The rdd.map() function can be used to perform an operation on every element in a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2018 Nissan Versa', '2018 Ford Fiesta', '2018 Hyundai Accent']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complex_data_df.rdd\\\n",
    "               .map(lambda x: (\"2018 \" + x.col_string))\\\n",
    "               .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding a column\n",
    "Add a new column which sums up the contents of the int and double columns. We do this using the withColumn() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-------+\n",
      "|col_int|col_double|col_sum|\n",
      "+-------+----------+-------+\n",
      "|    1.0|        12|   13.0|\n",
      "|    2.0|        13|   15.0|\n",
      "|    3.0|        15|   18.0|\n",
      "+-------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "complex_data_df.select(\n",
    "                       'col_int',\n",
    "                       'col_double'\n",
    "                      )\\\n",
    "               .withColumn(\n",
    "                       \"col_sum\",\n",
    "                        complex_data_df.col_int + complex_data_df.col_double\n",
    "                      )\\\n",
    "               .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+\n",
      "|col_bool|col_opposite|\n",
      "+--------+------------+\n",
      "|    true|       false|\n",
      "|    true|       false|\n",
      "|    true|       false|\n",
      "+--------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "complex_data_df.select('col_bool')\\\n",
    "               .withColumn(\n",
    "                           \"col_opposite\",\n",
    "                           complex_data_df.col_bool == False )\\\n",
    "               .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Editing a column name\n",
    "Make use of the withColumnRenamed() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+--------------+--------+--------------------+--------------------+---------+-------------------+\n",
      "|col_int|col_double|    col_string|col_bool|           col_array|             col_map|  col_row|      col_date_time|\n",
      "+-------+----------+--------------+--------+--------------------+--------------------+---------+-------------------+\n",
      "|    1.0|        12|  Nissan Versa|    true|           [2, 4, 6]|           [a1 -> 0]|[1, 2, 3]|2018-07-01 14:01:02|\n",
      "|    2.0|        13|   Ford Fiesta|    true|    [2, 4, 6, 8, 10]|  [a1 -> 0, a2 -> 1]|[1, 2, 3]|2018-07-02 14:01:03|\n",
      "|    3.0|        15|Hyundai Accent|    true|[2, 4, 6, 8, 10, ...|[a1 -> 0, a2 -> 1...|[1, 2, 3]|2018-07-03 14:01:04|\n",
      "+-------+----------+--------------+--------+--------------------+--------------------+---------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "complex_data_df.withColumnRenamed(\"col_dictionary\",\"col_map\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|  vehicle_name|\n",
      "+--------------+\n",
      "|  Nissan Versa|\n",
      "|   Ford Fiesta|\n",
      "|Hyundai Accent|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "complex_data_df.select(complex_data_df.col_string.alias(\"vehicle_name\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interoperablity between Pandas dataframe and Spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col_int</th>\n",
       "      <th>col_double</th>\n",
       "      <th>col_string</th>\n",
       "      <th>col_bool</th>\n",
       "      <th>col_array</th>\n",
       "      <th>col_dictionary</th>\n",
       "      <th>col_row</th>\n",
       "      <th>col_date_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>12</td>\n",
       "      <td>Nissan Versa</td>\n",
       "      <td>True</td>\n",
       "      <td>[2, 4, 6]</td>\n",
       "      <td>{'a1': 0}</td>\n",
       "      <td>(1, 2, 3)</td>\n",
       "      <td>2018-07-01 14:01:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>13</td>\n",
       "      <td>Ford Fiesta</td>\n",
       "      <td>True</td>\n",
       "      <td>[2, 4, 6, 8, 10]</td>\n",
       "      <td>{'a1': 0, 'a2': 1}</td>\n",
       "      <td>(1, 2, 3)</td>\n",
       "      <td>2018-07-02 14:01:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>15</td>\n",
       "      <td>Hyundai Accent</td>\n",
       "      <td>True</td>\n",
       "      <td>[2, 4, 6, 8, 10, 12, 14]</td>\n",
       "      <td>{'a1': 0, 'a2': 1, 'a3': 2}</td>\n",
       "      <td>(1, 2, 3)</td>\n",
       "      <td>2018-07-03 14:01:04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   col_int  col_double      col_string  col_bool                 col_array  \\\n",
       "0      1.0          12    Nissan Versa      True                 [2, 4, 6]   \n",
       "1      2.0          13     Ford Fiesta      True          [2, 4, 6, 8, 10]   \n",
       "2      3.0          15  Hyundai Accent      True  [2, 4, 6, 8, 10, 12, 14]   \n",
       "\n",
       "                col_dictionary    col_row       col_date_time  \n",
       "0                    {'a1': 0}  (1, 2, 3) 2018-07-01 14:01:02  \n",
       "1           {'a1': 0, 'a2': 1}  (1, 2, 3) 2018-07-02 14:01:03  \n",
       "2  {'a1': 0, 'a2': 1, 'a3': 2}  (1, 2, 3) 2018-07-03 14:01:04  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pandas = complex_data_df.toPandas()\n",
    "df_pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a Spark DataFrame from a Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+--------------+--------+--------------------+--------------------+---------+-------------------+\n",
      "|col_int|col_double|    col_string|col_bool|           col_array|      col_dictionary|  col_row|      col_date_time|\n",
      "+-------+----------+--------------+--------+--------------------+--------------------+---------+-------------------+\n",
      "|    1.0|        12|  Nissan Versa|    true|           [2, 4, 6]|           [a1 -> 0]|[1, 2, 3]|2018-07-01 14:01:02|\n",
      "|    2.0|        13|   Ford Fiesta|    true|    [2, 4, 6, 8, 10]|  [a1 -> 0, a2 -> 1]|[1, 2, 3]|2018-07-02 14:01:03|\n",
      "|    3.0|        15|Hyundai Accent|    true|[2, 4, 6, 8, 10, ...|[a1 -> 0, a2 -> 1...|[1, 2, 3]|2018-07-03 14:01:04|\n",
      "+-------+----------+--------------+--------+--------------------+--------------------+---------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark = sqlContext.createDataFrame(df_pandas).show()  \n",
    "df_spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
